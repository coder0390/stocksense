{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM, OVWeightQuantizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\lx\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "#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\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Haotianl/finephi2\"\n",
    "save_name = model_name.split(\"/\")[-1] + \"_openvino\"\n",
    "precision = \"f16\"\n",
    "quantization_config = OVWeightQuantizationConfig(\n",
    "    bits=4,\n",
    "    sym=False,\n",
    "    group_size=128,\n",
    "    ratio=0.8,\n",
    ")\n",
    "device = \"gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n"
     ]
    }
   ],
   "source": [
    "# Load kwargs\n",
    "load_kwargs = {\n",
    "    \"device\": device,\n",
    "    \"ov_config\": {\n",
    "        \"PERFORMANCE_HINT\": \"LATENCY\",\n",
    "        \"INFERENCE_PRECISION_HINT\": precision,\n",
    "        \"CACHE_DIR\": os.path.join(save_name, \"model_cache\"),  # OpenVINO will use this directory as cache\n",
    "    },\n",
    "    \"compile\": False,\n",
    "    \"quantization_config\": quantization_config\n",
    "}\n",
    "\n",
    "# Check whether the model was already exported\n",
    "saved = os.path.exists(save_name)\n",
    "\n",
    "model = OVModelForCausalLM.from_pretrained(\n",
    "    model_name if not saved else save_name,\n",
    "    export=not saved,\n",
    "    **load_kwargs,\n",
    ")\n",
    "\n",
    "# Load tokenizer to be used with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name if not saved else save_name)\n",
    "\n",
    "# Save the exported model locally\n",
    "if not saved:\n",
    "    model.save_pretrained(save_name)\n",
    "    tokenizer.save_pretrained(save_name)\n",
    "\n",
    "# TODO Optional: export to huggingface/hub\n",
    "\n",
    "model_size = os.stat(os.path.join(save_name, \"openvino_model.bin\")).st_size / 1024 ** 3\n",
    "print(f'Model size in FP32: ~5.4GB, current model size in 4bit: {model_size:.2f}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied and modified from https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "from transformers import (\n",
    "    TextIteratorStreamer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    GenerationConfig,\n",
    ")\n",
    "\n",
    "\n",
    "# Copied and modified from https://github.com/bigcode-project/bigcode-evaluation-harness/blob/main/bigcode_eval/generation.py#L13\n",
    "class SuffixCriteria(StoppingCriteria):\n",
    "    def __init__(self, start_length, eof_strings, tokenizer, check_fn=None):\n",
    "        self.start_length = start_length\n",
    "        self.eof_strings = eof_strings\n",
    "        self.tokenizer = tokenizer\n",
    "        if check_fn is None:\n",
    "            check_fn = lambda decoded_generation: any(\n",
    "                [decoded_generation.endswith(stop_string) for stop_string in self.eof_strings]\n",
    "            )\n",
    "        self.check_fn = check_fn\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        \"\"\"Returns True if generated sequence ends with any of the stop strings\"\"\"\n",
    "        decoded_generations = self.tokenizer.batch_decode(input_ids[:, self.start_length :])\n",
    "        return all([self.check_fn(decoded_generation) for decoded_generation in decoded_generations])\n",
    "\n",
    "\n",
    "def is_partial_stop(output, stop_str):\n",
    "    \"\"\"Check whether the output contains a partial stop str.\"\"\"\n",
    "    for i in range(0, min(len(output), len(stop_str))):\n",
    "        if stop_str.startswith(output[-i:]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Set the chat template to the tokenizer. The chat template implements the simple template of\n",
    "#   User: content\n",
    "#   Assistant: content\n",
    "#   ...\n",
    "# Read more about chat templates here https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "tokenizer.chat_template = \"{% for message in messages %}{{message['role'] + ': ' + message['content'] + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\"\n",
    "\n",
    "\n",
    "def prepare_history_for_model(history):\n",
    "    \"\"\"\n",
    "    Converts the history to a tokenized prompt in the format expected by the model.\n",
    "    Params:\n",
    "      history: dialogue history\n",
    "    Returns:\n",
    "      Tokenized prompt\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        # skip the last assistant message if its empty, the tokenizer will do the formating\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"User\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"User\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"Assistant\", \"content\": model_msg})\n",
    "    input_token = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True\n",
    "    )\n",
    "    return input_token\n",
    "\n",
    "\n",
    "def generate(history, temperature, max_new_tokens, top_p, repetition_penalty, assisted):\n",
    "    \"\"\"\n",
    "    Generates the assistant's reponse given the chatbot history and generation parameters\n",
    "\n",
    "    Params:\n",
    "      history: conversation history formated in pairs of user and assistant messages `[user_message, assistant_message]`\n",
    "      temperature:  parameter for control the level of creativity in AI-generated text.\n",
    "                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n",
    "      max_new_tokens: The maximum number of tokens we allow the model to generate as a response.\n",
    "      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n",
    "      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n",
    "      assisted: boolean parameter to enable/disable assisted generation with speculative decoding.\n",
    "    Yields:\n",
    "      Updated history and generation status.\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    # Construct the input message string for the model by concatenating the current system message and conversation history\n",
    "    # Tokenize the messages string\n",
    "    inputs = prepare_history_for_model(history)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    # truncate input in case it is too long.\n",
    "    # TODO improve this\n",
    "    if input_length > 2000:\n",
    "        history = [history[-1]]\n",
    "        inputs = prepare_history_for_model(history)\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "    prompt_char = \"â–Œ\"\n",
    "    history[-1][1] = prompt_char\n",
    "    yield history, \"Status: Generating...\", *([gr.update(interactive=False)] * 4)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # Create a stopping criteria to prevent the model from playing the role of the user aswell.\n",
    "    stop_str = [\"\\nUser:\", \"\\nAssistant:\", \"\\nRules:\", \"\\nQuestion:\"]\n",
    "    stopping_criteria = StoppingCriteriaList([SuffixCriteria(input_length, stop_str, tokenizer)])\n",
    "    # Prepare input for generate\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=temperature > 0.0,\n",
    "        temperature=temperature if temperature > 0.0 else 1.0,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=[tokenizer.eos_token_id],\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    generate_kwargs = dict(\n",
    "        streamer=streamer,\n",
    "        generation_config=generation_config,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "       **inputs\n",
    "    )\n",
    "\n",
    "    if assisted:\n",
    "        target_generate = stateless_model.generate\n",
    "        generate_kwargs[\"assistant_model\"] = asst_model\n",
    "    else:\n",
    "        target_generate = model.generate\n",
    "\n",
    "    t1 = Thread(target=target_generate, kwargs=generate_kwargs)\n",
    "    t1.start()\n",
    "\n",
    "    # Initialize an empty string to store the generated text.\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text += new_text\n",
    "        history[-1][1] = partial_text + prompt_char\n",
    "        for s in stop_str:\n",
    "            if (pos := partial_text.rfind(s)) != -1:\n",
    "                break\n",
    "        if pos != -1:\n",
    "            partial_text = partial_text[:pos]\n",
    "            break\n",
    "        elif any([is_partial_stop(partial_text, s) for s in stop_str]):\n",
    "            continue\n",
    "        yield history, \"Status: Generating...\", *([gr.update(interactive=False)] * 4)\n",
    "    history[-1][1] = partial_text\n",
    "    generation_time = time.perf_counter() - start\n",
    "    yield history, f'Generation time: {generation_time:.2f} sec', *([gr.update(interactive=True)] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied and modified from https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb\n",
    "import gradio as gr\n",
    "\n",
    "try:\n",
    "    demo.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "EXAMPLES = [\n",
    "    [\"What can you help me?\"],\n",
    "    [\"Can you explain to me briefly what is LSTM model?\"],\n",
    "    [\"Explain what is Interest Rate.\"],\n",
    "    [\"Provide some Investment Portfolio\"],\n",
    "    [\"Lily has a rubber ball that she drops from the top of a wall. The wall is 2 meters tall. How long will it take for the ball to reach the ground?\"],\n",
    "]\n",
    "\n",
    "\n",
    "def add_user_text(message, history):\n",
    "    \"\"\"\n",
    "    Add user's message to chatbot history\n",
    "\n",
    "    Params:\n",
    "      message: current user message\n",
    "      history: conversation history\n",
    "    Returns:\n",
    "      Updated history, clears user message and status\n",
    "    \"\"\"\n",
    "    # Append current user message to history with a blank assistant message which will be generated by the model\n",
    "    history.append([message, None])\n",
    "    return ('', history)\n",
    "\n",
    "\n",
    "def prepare_for_regenerate(history):\n",
    "    \"\"\"\n",
    "    Delete last assistant message to prepare for regeneration\n",
    "\n",
    "    Params:\n",
    "      history: conversation history\n",
    "    Returns:\n",
    "      updated history\n",
    "    \"\"\" \n",
    "    history[-1][1] = None\n",
    "    return history\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Glass()) as demo:\n",
    "    # Add custom icon before title\n",
    "    icon_url = 'https://i.ibb.co/1Lh15Jy/R.png'\n",
    "    icon_tag = f'<img src=\"{icon_url}\" style=\"height: 100px; margin-right: 20px;\">'\n",
    "    title_tag = '<h1 style=\"text-align: center;\">StockSence Intelligent helper</h1>'\n",
    "    header_tag = f'<div style=\"display: flex; align-items: center;\">{icon_tag}{title_tag}</div>'\n",
    "    gr.Markdown(header_tag)\n",
    "    #gr.Markdown('<h1 style=\"text-align: center;\">StockSence Intelligent helper</h1>')\n",
    "    chatbot = gr.Chatbot()\n",
    "    with gr.Row():\n",
    "        #assisted = gr.Checkbox(value=False, label=\"Assisted Generation\", scale=10)\n",
    "        msg = gr.Textbox(placeholder=\"Enter message here...\", show_label=False, autofocus=True, scale=75)\n",
    "        status = gr.Textbox(\"Status: Idle\", show_label=False, max_lines=1, scale=15)\n",
    "    with gr.Row():\n",
    "        submit = gr.Button(\"Submit\", variant='primary')\n",
    "        #regenerate = gr.Button(\"Regenerate\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "    with gr.Accordion(\"Advanced Options:\", open=False):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                temperature = gr.Slider(\n",
    "                    label=\"Temperature\",\n",
    "                    value=0.0,\n",
    "                    minimum=0.0,\n",
    "                    maximum=1.0,\n",
    "                    step=0.05,\n",
    "                    interactive=True,\n",
    "                )\n",
    "                max_new_tokens = gr.Slider(\n",
    "                    label=\"Max new tokens\",\n",
    "                    value=128,\n",
    "                    minimum=0,\n",
    "                    maximum=512,\n",
    "                    step=32,\n",
    "                    interactive=True,\n",
    "                )\n",
    "            with gr.Column():\n",
    "                top_p = gr.Slider(\n",
    "                    label=\"Top-p (nucleus sampling)\",\n",
    "                    value=1.0,\n",
    "                    minimum=0.0,\n",
    "                    maximum=1.0,\n",
    "                    step=0.05,\n",
    "                    interactive=True,\n",
    "                )\n",
    "                repetition_penalty = gr.Slider(\n",
    "                    label=\"Repetition penalty\",\n",
    "                    value=1.0,\n",
    "                    minimum=1.0,\n",
    "                    maximum=2.0,\n",
    "                    step=0.1,\n",
    "                    interactive=True,\n",
    "                )\n",
    "    gr.Examples(\n",
    "        EXAMPLES, inputs=msg, label=\"Click on any example and press the 'Submit' button\"\n",
    "    )\n",
    "\n",
    "    # Sets generate function to be triggered when the user submit a new message\n",
    "    gr.on(\n",
    "        triggers=[submit.click, msg.submit],\n",
    "        fn=add_user_text,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=generate,\n",
    "        inputs=[chatbot, temperature, max_new_tokens, top_p, repetition_penalty],\n",
    "        outputs=[chatbot, status, msg, submit, clear],#, regenerate\n",
    "        concurrency_limit=1,\n",
    "        queue=True\n",
    "    )\n",
    "    #regenerate.click(\n",
    "    #    fn=prepare_for_regenerate,\n",
    "    #    inputs=chatbot,\n",
    "    #    outputs=chatbot,\n",
    "    #    queue=True,\n",
    "    #    concurrency_limit=1\n",
    "    #).then(\n",
    "    #    fn=generate,\n",
    "    #    inputs=[chatbot, temperature, max_new_tokens, top_p, repetition_penalty],\n",
    "    #    outputs=[chatbot, status, msg, submit, regenerate, clear],\n",
    "    #    concurrency_limit=1,\n",
    "    #    queue=True\n",
    "    #)\n",
    "    clear.click(fn=lambda: (None, \"Status: Idle\"), inputs=None, outputs=[chatbot, status], queue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
